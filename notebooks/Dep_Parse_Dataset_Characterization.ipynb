{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author: ddukic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy RDRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import re\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r\"\\S+\").match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_left_right_deps(\n",
    "    tags=[\"B-Person\", \"I-Person\", \"O\", \"O\", \"O\", \"B-Location\", \"O\"],\n",
    "    sent_start=\"Barack Obama was born in Hawaii .\",\n",
    "):\n",
    "    left_count = 0\n",
    "    right_count = 0\n",
    "\n",
    "    doc = nlp(sent_start)\n",
    "\n",
    "    for word, tag in zip(doc, tags):\n",
    "        # print(word, word.i, word.head, word.head.i, [w for w in word.children])\n",
    "        if word.dep_ != \"ROOT\":\n",
    "            if word.head.i > word.i and tag != \"O\":\n",
    "                right_count += 1\n",
    "            elif word.head.i < word.i and tag != \"O\":\n",
    "                left_count += 1\n",
    "            else:\n",
    "                if tag != \"O\":\n",
    "                    print(\"Something is wrong!\")\n",
    "\n",
    "        for child in word.children:\n",
    "            if child.i > word.i and tag != \"O\":\n",
    "                right_count += 1\n",
    "            elif child.i < word.i and tag != \"O\":\n",
    "                left_count += 1\n",
    "            else:\n",
    "                if tag != \"O\":\n",
    "                    print(\"Something is wrong!\")\n",
    "\n",
    "    return left_count, right_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_left_right_deps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [00:50<00:00, 278.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003 0.593070475198063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14041/14041 [00:49<00:00, 284.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conll2003chunk 0.5180003314872921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14672/14672 [00:55<00:00, 263.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ace-tc 0.4109069886947585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2737/2737 [00:10<00:00, 272.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absa-restaurants 0.3962785556374355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../baselines/\")\n",
    "from transformers import AutoTokenizer\n",
    "from dataset import TokenClassificationDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "\n",
    "\n",
    "for ds_name in [\"conll2003\", \"conll2003chunk\", \"ace-tc\", \"absa-restaurants\"]:\n",
    "    dataset = TokenClassificationDataset(ds_name, tokenizer, \"train\")\n",
    "\n",
    "    left_global, right_global = 0, 0\n",
    "\n",
    "    for tags, sentence in tqdm(\n",
    "        zip(dataset.labels, dataset.tokens), total=len(dataset.labels)\n",
    "    ):\n",
    "        l, r = count_left_right_deps(tags, \" \".join(sentence))\n",
    "        left_global += l\n",
    "        right_global += r\n",
    "\n",
    "    print(ds_name, right_global / (left_global + right_global))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
